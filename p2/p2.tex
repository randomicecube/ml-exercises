\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=1.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage{svg}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage{nicefrac}
\usepackage{multirow}
\usepackage[most]{tcolorbox}
\usepackage[colorlinks=true]{hyperref}

\setlength{\droptitle}{-6em}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{bg}{rgb}{1,0.96,0.9}

\lstdefinestyle{mystyle}{
  commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2
}

% \tcbset{enlarge left by=-0.8cm,left=1.2cm,enlarge right by=-2cm,right=0.8cm}

\lstset{
  style=mystyle,
  inputencoding=utf8,
  extendedchars=true,
}

\newcommand{\question}[1]{\begin{tcolorbox}[enhanced jigsaw,colback=bg,boxrule=0pt,arc=1pt,halign=center] #1 \end{tcolorbox}}

\begin{document}

\begin{enumerate}[leftmargin=\labelsep]
  \question {
  \item Consider the following dataset:

        \begin{table}[H]
          \centering
          \begin{tabular}{c|c|c|c|c}
                  & $y_1$ & $y_2$ & $y_3$ & $z$ \\ \hline
            $x_1$ & a     & a     & a     & +   \\
            $x_2$ & c     & b     & c     & +   \\
            $x_3$ & c     & a     & c     & +   \\
            $x_4$ & b     & a     & a     & -   \\
            $x_5$ & a     & b     & c     & -   \\
            $x_6$ & b     & b     & c     & -
          \end{tabular}
        \end{table}

        Plot the learned decision tree using information gain (Shannon entropy). Show your calculations.
        }

        Intuitively, the amount of information we can gather upon observing a random event
        is inversely proportional to the probability of it happening. Shannon's notion
        of \textbf{entropy} is a formalization of this idea. The entropy of a random variable
        is the average amount of information we can gather upon observing it, and is defined as follows:

        \begin{equation*}
          H(X) = \sum_{x \in \mathcal{X}} p(x) \log_2 {\frac{1}{p(x)}} = -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)
        \end{equation*}

        Here, $\mathcal{X}$ is the set of all possible values of $X$ and $p(x)$ is the
        probability of $X$ taking the value $x$.

        The \textbf{information gain} concept is a measure of how much information we gain
        by observing a random variable $X$ - the larger the information gain value,
        the more we can extract from a given feature for the given dataset. It is defined as the difference between the entropy
        of the random variable $X$ and the \textit{conditional entropy} of $X$ given $Y$:
        a measure of how much information is needed to describe $X$ given that we know $Y$.

        \begin{equation*}
          \text{IG}(X, Y) = H(X) - H(X|Y)
        \end{equation*}

        Having these definitions in mind, it should now be trivial to plot the decision tree
        for the dataset above. We start by computing the entropy of the target variable $z$:

        \begin{equation*}
          H(z) = -\frac{3}{6} \log_2 \frac{3}{6} - \frac{3}{6} \log_2 \frac{3}{6} = 1
        \end{equation*}

        Learning a decision tree will be intrinsically related to the features' information gain
        measure: for each level, we will select the feature that maximizes the information gain
        with respect to the target variable, making it that level's decision node.
        Let's compute the information gain for each feature:

        \begin{equation*}
          \begin{aligned}
            H(z \mid y_1) = \sum_{y \in \mathcal{Y}_1} p(y_1 = y) H(z \mid y_1 = y)
            = \frac{2}{6} \left(-\frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2}\right)
            + \frac{2}{6} \left(-\frac{2}{2} \log_2 \frac{2}{2}\right)
            + \frac{2}{6} \left(-\frac{2}{2} \log_2 \frac{2}{2}\right)
            = \mathbf{\frac{1}{3}}
          \end{aligned}
        \end{equation*}

        \begin{equation*}
          IG(z, y_1)    = 1 - \frac{1}{3} = \mathbf{\frac{2}{3}}
        \end{equation*}

        Performing similar computations for the other features, we obtain:

        \begin{equation*}
          \begin{aligned}
             & H(z \mid y_2) = 0.9183, \quad &  & IG(z, y_2) = 0.082 \\
             & H(z \mid y_3) = 1, \quad      &  & IG(z, y_3) = 0
          \end{aligned}
        \end{equation*}

        As mentioned above, we will select the feature that maximizes the information gain
        for each level. In this case, $y_1$ is the feature that maximizes the information gain,
        hence it will be the root node of the decision tree. Note how there are already
        a couple of leaves on our tree: these indicate that, regarding the respective
        decision tree node, the target variable is already fully determined by the feature
        values (i.e for a given feature value, the target variable is always the same).

        Continuing the process, and following the path for the feature $y_1$ that takes the value $a$,
        we can choose either $y_2$ or $y_3$ as the next decision node, since the
        entropies of $y_2$ and $y_3$ are the same, zero (considering data conditioned by $y_1 = a$).
        Choosing $y_2$ as the next decision node, we're now left with only lead nodes,
        therefore no further decision nodes should be added to the tree.

        \begin{figure}[H]
          \centering
          \includesvg[width=0.2\textwidth]{assets/ex-1-decision-tree.svg}
        \end{figure}

        \question {
  \item Show if a decision tree can learn the AND, OR and XOR logical functions.
        }

        Note that the afore-mentioned logical functions can be represented as shown
        in the following "dummy data sets":

        % three tables, one for each logical function, side-by-side

        \begin{table}[!htb]
          \begin{minipage}{.333\linewidth}
            \centering
            \begin{tabular}{c|c|c|c}
                    & $y_1$ & $y_2$ & $z$ \\ \hline
              $x_1$ & 0     & 0     & 0   \\
              $x_2$ & 0     & 1     & 0   \\
              $x_3$ & 1     & 0     & 0   \\
              $x_4$ & 1     & 1     & 1
            \end{tabular}
            \caption{AND logical function}
          \end{minipage}\hfill
          \begin{minipage}{.333\linewidth}
            \centering
            \begin{tabular}{c|c|c|c}
                    & $y_1$ & $y_2$ & $z$ \\ \hline
              $x_1$ & 0     & 0     & 0   \\
              $x_2$ & 0     & 1     & 1   \\
              $x_3$ & 1     & 0     & 1   \\
              $x_4$ & 1     & 1     & 1
            \end{tabular}
            \caption{OR logical function}
          \end{minipage}\hfill
          \begin{minipage}{.333\linewidth}
            \centering
            \begin{tabular}{c|c|c|c}
                    & $y_1$ & $y_2$ & $z$ \\ \hline
              $x_1$ & 0     & 0     & 0   \\
              $x_2$ & 0     & 1     & 1   \\
              $x_3$ & 1     & 0     & 1   \\
              $x_4$ & 1     & 1     & 0
            \end{tabular}
            \caption{XOR logical function}
          \end{minipage}
        \end{table}

        Each one of these data sets can be learned by a decision tree, of course:

        \begin{figure}[H]
          \centering
          \includesvg[width=\textwidth]{assets/ex-2-trees.svg}
        \end{figure}

        \question {
  \item Consider the following testing targets, $z$, and the corresponding predictions, $\hat{z}$,
        by a decision tree:

        \begin{equation*}
          \begin{aligned}
            z       & = [A, A, A, B, B, B, C, C, C, C] \\
            \hat{z} & = [B, B, A, C, B, A, C, A, B, C]
          \end{aligned}
        \end{equation*}

        \begin{enumerate}
          \item Draw the confusion matrix.
          \item Compute the accuracy and recall (sensitivity) for each class.
          \item Regarding class C, identify its precision and F-measure.
          \item Identify the accuracy, sensitivity and precision of a random classifier.
        \end{enumerate}
        }

        As we know, a confusion matrix is a $K \times K$ matrix, where $K$ is the number of
        classes in our target label, which aims to aid in the visualization of the performance
        of a (usually supervised) classifier. In this case, we have three classes, $A$, $B$ and $C$,
        therefore our confusion matrix will be a $3 \times 3$ matrix. Considering the
        \textit{real vs predicted} labels referenced above, we can fill the confusion matrix
        as follows:

        \begin{figure}[H]
          \centering
          \includesvg[width=0.35\textwidth]{assets/ex-3-confusion-matrix.svg}
        \end{figure}

        The \textbf{accuracy} of a classifier is the proportion of correct predictions made by the
        classifier. In this case, we have 4 correct predictions (given by the matrix' trace),
        out of 10 total predictions. Therefore, the accuracy is, of course, $4/10 = 0.4$.

        The \textbf{recall} (also known as \textbf{sensitivity}) measures the proportion
        of correctly $X$-predicted instances out of all instances of class $X$.
        Measuring the recall for each class, we have the following:

        \begin{equation*}
          \operatorname{recall}(A) = \frac{1}{1 + 2 + 0} = \frac{1}{3}, \quad
          \operatorname{recall}(B) = \frac{1}{1 + 1 + 1} = \frac{1}{3}, \quad
          \operatorname{recall}(C) = \frac{2}{1 + 1 + 2} = \frac{1}{2}
        \end{equation*}

        The \textbf{precision} is essentially a class-wise version of the accuracy
        measure. It measures the proportion of correctly $X$-predicted instances out of all
        instances predicted as $X$. Measuring the precision for each class, we have the following:

        \begin{equation*}
          \operatorname{precision}(A) = \frac{1}{1 + 1 + 1} = \frac{1}{3}, \quad
          \operatorname{precision}(B) = \frac{1}{2 + 1 + 1} = \frac{1}{4}, \quad
          \operatorname{precision}(C) = \frac{2}{0 + 1 + 2} = \frac{2}{3}
        \end{equation*}

        These measures can be more easily visualized as shown in the following figure:

        \begin{figure}[H]
          \centering
          \includesvg[width=0.5\textwidth]{assets/ex-3-measures.svg}
        \end{figure}

        The \textbf{F-measure} is the harmonic mean of the precision and recall, and is
        defined as:

        \begin{equation*}
          F_\beta = \frac{(1 + \beta^2) \cdot \operatorname{precision} \cdot \operatorname{recall}}
          {\beta^2 \cdot \operatorname{precision} + \operatorname{recall}}
        \end{equation*}

        In this case, we have $\beta = 1$ (since the question's statement asks us
        for the F1-measure). As such, we'll have the following:

        \begin{equation*}
          F_{1,a} = \frac{2 \cdot \operatorname{precision}_a \cdot \operatorname{recall}_a}
          {\operatorname{precision}_a + \operatorname{recall}_a}
          = \frac{2 \cdot \frac{1}{3} \cdot \frac{1}{3}}{\frac{1}{3} + \frac{1}{3}}
          = \frac{1}{3}, \quad
          F_{1,b} = \frac{2 \cdot \frac{1}{4} \cdot \frac{1}{3}}{\frac{1}{4} + \frac{1}{3}}
          = \frac{2}{7}, \quad
          F_{1,c} = \frac{2 \cdot \frac{2}{3} \cdot \frac{1}{2}}{\frac{2}{3} + \frac{1}{2}}
          = 0.5714
        \end{equation*}

        Finally, let's talk about the \textbf{random classifier}. A random classifier
        is a classifier that makes random predictions, and is usually used as a baseline
        for comparison with other classifiers. In this case, we have three classes, $A$, $B$ and $C$,
        therefore the random classifier will predict each class with probability $\frac{1}{3}$.
        As such, the accuracy of the random classifier is also $\frac{1}{3}$.

        Regarding both sensitivity and precision, we have the following (considering
        $X$ as the class predicted by the random classifier, $\overline{X}$ as all
        the remaining classes, and $p = P(X)$):

        % TODO: change this (apparently) wrong

        \begin{equation*}
          \operatorname{recall}(X) = \frac{TX}{TX + F\overline{X}} = \frac{p \cdot X}{p \cdot X + (1 - p) X} = p, \quad
          \operatorname{precision}(X) = \frac{TX}{TX + FX} = \frac{p \cdot X}{p \cdot X + (1 - p) \overline{X}}
        \end{equation*}

        Putting it into words, $TX$ is the number of correct $X$ predictions - considering
        a random classifier, and $X$ amount of $X$ instances, there'll be $p \cdot X$ correct
        $X$ predictions, of course.

        $F\overline{X}$ is the number of incorrect non-$X$ guesses - that is, the
        amount of times we should have predicted $X$ but didn't. Thinking about concrete
        examples, in the random classifier in hands, with $p = \nicefrac{1}{3}$, it's
        intuitive that we'll be able to correctly predict $X$ $p \cdot X = \nicefrac{1}{3} \cdot X$
        times, but we'll also not be able to predict $X$ $1 - p = \nicefrac{2}{3}$ times:
        for each time we predict $X$ correctly, we'll miss two other times.

        Finally, $FX$ is the number of incorrect $X$ guesses - that is, the amount of
        times we should have predicted $\overline{X}$ but didn't. In an analogous way,
        way to $F\overline{X}$, we'll have $FX = (1 - p) \cdot \overline{X}$.

        Note that, considering the formulas above, the recall for all classes is given
        by $p = \nicefrac{1}{3}$, while the precision differs between classes:

        \begin{equation*}
          \operatorname{precision}(A) = \operatorname{precision}(B) = \frac{\nicefrac{1}{3} \cdot 3}{\nicefrac{1}{3} \cdot 3 + \nicefrac{2}{3} \cdot 7} = \frac{3}{17}, \quad
          \operatorname{precision}(C) = \frac{\nicefrac{1}{3} \cdot 4}{\nicefrac{1}{3} \cdot 4 + \nicefrac{2}{3} \cdot 6} = \frac{1}{4}
        \end{equation*}

        \question {
  \item Consider a dataset composed by 374 records, described by 6 variables, classified
        according to the following decision tree:

        \begin{figure}[H]
          \centering
          \includesvg[width=0.5\textwidth]{assets/ex-4-tree.svg}
        \end{figure}

        Each leaf in the tree shows the label, number of classified records with the
        label, and total number of observations in the leaf. The positive class is
        the minority class.

        \begin{enumerate}
          \item Compute the confusion matrix.
          \item Compare the accuracy of the given tree versus a pruned tree, with
                only two nodes. Is there any evidence towards overfitting?
          \item Are decision trees learned from high-dimensional data susceptible
                to underfitting? Why does an ensemble of DTs minimize this problem?
        \end{enumerate}
        }

\end{enumerate}

\end{document}