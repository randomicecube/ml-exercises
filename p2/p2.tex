\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=1.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage{svg}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage{nicefrac}
\usepackage{multirow}
\usepackage[most]{tcolorbox}
\usepackage[colorlinks=true]{hyperref}

\setlength{\droptitle}{-6em}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{bg}{rgb}{1,0.96,0.9}

\lstdefinestyle{mystyle}{
  commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2
}

% \tcbset{enlarge left by=-0.8cm,left=1.2cm,enlarge right by=-2cm,right=0.8cm}

\lstset{
  style=mystyle,
  inputencoding=utf8,
  extendedchars=true,
}

\newcommand{\question}[1]{\begin{tcolorbox}[enhanced jigsaw,colback=bg,boxrule=0pt,arc=1pt,halign=center] #1 \end{tcolorbox}}

\begin{document}

\begin{enumerate}[leftmargin=\labelsep]
  \question {
  \item Consider the following dataset:

        \begin{table}[H]
          \centering
          \begin{tabular}{c|c|c|c|c}
                  & $y_1$ & $y_2$ & $y_3$ & $z$ \\ \hline
            $x_1$ & a     & a     & a     & +   \\
            $x_2$ & c     & b     & c     & +   \\
            $x_3$ & c     & a     & c     & +   \\
            $x_4$ & b     & a     & a     & -   \\
            $x_5$ & a     & b     & c     & -   \\
            $x_6$ & b     & b     & c     & -
          \end{tabular}
        \end{table}

        Plot the learned decision tree using information gain (Shannon entropy). Show your calculations.
        }

        Intuitively, the amount of information we can gather upon observing a random event
        is inversely proportional to the probability of it happening. Shannon's notion
        of \textbf{entropy} is a formalization of this idea. The entropy of a random variable
        is the average amount of information we can gather upon observing it, and is defined as follows:

        \begin{equation*}
          H(X) = \sum_{x \in \mathcal{X}} p(x) \log_2 {\frac{1}{p(x)}} = -\sum_{x \in \mathcal{X}} p(x) \log_2 p(x)
        \end{equation*}

        Here, $\mathcal{X}$ is the set of all possible values of $X$ and $p(x)$ is the
        probability of $X$ taking the value $x$.

        The \textbf{information gain} concept is a measure of how much information we gain
        by observing a random variable $X$ - the larger the information gain value,
        the more we can extract from a given feature for the given dataset. It is defined as the difference between the entropy
        of the random variable $X$ and the \textit{conditional entropy} of $X$ given $Y$:
        a measure of how much information is needed to describe $X$ given that we know $Y$.

        \begin{equation*}
          \text{IG}(X, Y) = H(X) - H(X|Y)
        \end{equation*}

        Having these definitions in mind, it should now be trivial to plot the decision tree
        for the dataset above. We start by computing the entropy of the target variable $z$:

        \begin{equation*}
          H(z) = -\frac{3}{6} \log_2 \frac{3}{6} - \frac{3}{6} \log_2 \frac{3}{6} = 1
        \end{equation*}

        Learning a decision tree will be intrinsically related to the features' information gain
        measure: for each level, we will select the feature that maximizes the information gain
        with respect to the target variable, making it that level's decision node.
        Let's compute the information gain for each feature:

        \begin{equation*}
          \begin{aligned}
            H(z \mid y_1) = \sum_{y \in \mathcal{Y}_1} p(y_1 = y) H(z \mid y_1 = y)
            = \frac{2}{6} \left(-\frac{1}{2} \log_2 \frac{1}{2} - \frac{1}{2} \log_2 \frac{1}{2}\right)
            + \frac{2}{6} \left(-\frac{2}{2} \log_2 \frac{2}{2}\right)
            + \frac{2}{6} \left(-\frac{2}{2} \log_2 \frac{2}{2}\right)
            = \mathbf{\frac{1}{3}}
          \end{aligned}
        \end{equation*}

        \begin{equation*}
          IG(z, y_1)    = 1 - \frac{1}{3} = \mathbf{\frac{2}{3}}
        \end{equation*}

        Performing similar computations for the other features, we obtain:

        \begin{equation*}
          \begin{aligned}
             & H(z \mid y_2) = 0.9183, \quad &  & IG(z, y_2) = 0.082 \\
             & H(z \mid y_3) = 1, \quad      &  & IG(z, y_3) = 0
          \end{aligned}
        \end{equation*}

        As mentioned above, we will select the feature that maximizes the information gain
        for each level. In this case, $y_1$ is the feature that maximizes the information gain,
        hence it will be the root node of the decision tree. Note how there are already
        a couple of leaves on our tree: these indicate that, regarding the respective
        decision tree node, the target variable is already fully determined by the feature
        values (i.e for a given feature value, the target variable is always the same).

        Continuing the process, and following the path for the feature $y_1$ that takes the value $a$,
        we can choose either $y_2$ or $y_3$ as the next decision node, since the
        entropies of $y_2$ and $y_3$ are the same, zero (considering data conditioned by $y_1 = a$).
        Choosing $y_2$ as the next decision node, we're now left with only lead nodes,
        therefore no further decision nodes should be added to the tree.

        \begin{figure}[H]
          \centering
          \includesvg[width=0.2\textwidth]{assets/ex-1-decision-tree.svg}
        \end{figure}

        \question {
  \item Show if a decision tree can learn the AND, OR and XOR logical functions.
        }

        Note that the afore-mentioned logical functions can be represented as shown
        in the following "dummy data sets":

        % three tables, one for each logical function, side-by-side

        \begin{table}[!htb]
          \begin{minipage}{.333\linewidth}
            \centering
            \begin{tabular}{c|c|c|c}
                    & $y_1$ & $y_2$ & $z$ \\ \hline
              $x_1$ & 0     & 0     & 0   \\
              $x_2$ & 0     & 1     & 0   \\
              $x_3$ & 1     & 0     & 0   \\
              $x_4$ & 1     & 1     & 1
            \end{tabular}
            \caption{AND logical function}
          \end{minipage}\hfill
          \begin{minipage}{.333\linewidth}
            \centering
            \begin{tabular}{c|c|c|c}
                    & $y_1$ & $y_2$ & $z$ \\ \hline
              $x_1$ & 0     & 0     & 0   \\
              $x_2$ & 0     & 1     & 1   \\
              $x_3$ & 1     & 0     & 1   \\
              $x_4$ & 1     & 1     & 1
            \end{tabular}
            \caption{OR logical function}
          \end{minipage}\hfill
          \begin{minipage}{.333\linewidth}
            \centering
            \begin{tabular}{c|c|c|c}
                    & $y_1$ & $y_2$ & $z$ \\ \hline
              $x_1$ & 0     & 0     & 0   \\
              $x_2$ & 0     & 1     & 1   \\
              $x_3$ & 1     & 0     & 1   \\
              $x_4$ & 1     & 1     & 0
            \end{tabular}
            \caption{XOR logical function}
          \end{minipage}
        \end{table}

        Each one of these data sets can be learned by a decision tree, of course:

        \begin{figure}[H]
          \centering
          \includesvg[width=\textwidth]{assets/ex-2-trees.svg}
        \end{figure}

        \question {
  \item Consider the following testing targets, $z$, and the corresponding predictions, $\hat{z}$,
        by a decision tree:

        \begin{equation*}
          \begin{aligned}
            z       & = [A, A, A, B, B, B, C, C, C, C] \\
            \hat{z} & = [B, B, A, C, B, A, C, A, B, C]
          \end{aligned}
        \end{equation*}

        \begin{enumerate}
          \item Draw the confusion matrix.
          \item Compute the accuracy and recall (sensitivity) for each class.
          \item Regarding class C, identify its precision and F-measure.
          \item Identify the accuracy, sensitivity and precision of a random classifier.
        \end{enumerate}
        }

        As we know, a confusion matrix is a $K \times K$ matrix, where $K$ is the number of
        classes in our target label, which aims to aid in the visualization of the performance
        of a (usually supervised) classifier. In this case, we have three classes, $A$, $B$ and $C$,
        therefore our confusion matrix will be a $3 \times 3$ matrix. Considering the
        \textit{real vs predicted} labels referenced above, we can fill the confusion matrix
        as follows:

        \begin{figure}[H]
          \centering
          \includesvg[width=0.35\textwidth]{assets/ex-3-confusion-matrix.svg}
        \end{figure}

        The \textbf{accuracy} of a classifier is the proportion of correct predictions made by the
        classifier. In this case, we have 4 correct predictions (given by the matrix' trace),
        out of 10 total predictions. Therefore, the accuracy is, of course, $4/10 = 0.4$.

        The \textbf{recall} (also known as \textbf{sensitivity}) measures the proportion
        of correctly $X$-predicted instances out of all instances of class $X$.
        Measuring the recall for each class, we have the following:

        \begin{equation*}
          \operatorname{recall}(A) = \frac{1}{1 + 2 + 0} = \frac{1}{3}, \quad
          \operatorname{recall}(B) = \frac{1}{1 + 1 + 1} = \frac{1}{3}, \quad
          \operatorname{recall}(C) = \frac{2}{1 + 1 + 2} = \frac{1}{2}
        \end{equation*}

        The \textbf{precision} is essentially a class-wise version of the accuracy
        measure. It measures the proportion of correctly $X$-predicted instances out of all
        instances predicted as $X$. Measuring the precision for each class, we have the following:

        \begin{equation*}
          \operatorname{precision}(A) = \frac{1}{1 + 1 + 1} = \frac{1}{3}, \quad
          \operatorname{precision}(B) = \frac{1}{2 + 1 + 1} = \frac{1}{4}, \quad
          \operatorname{precision}(C) = \frac{2}{0 + 1 + 2} = \frac{2}{3}
        \end{equation*}

        These measures can be more easily visualized as shown in the following figure:

        \begin{figure}[H]
          \centering
          \includesvg[width=0.5\textwidth]{assets/ex-3-measures.svg}
        \end{figure}

        The \textbf{F-measure} is the harmonic mean of the precision and recall, and is
        defined as:

        \begin{equation*}
          F_\beta = \frac{(1 + \beta^2) \cdot \operatorname{precision} \cdot \operatorname{recall}}
          {\beta^2 \cdot \operatorname{precision} + \operatorname{recall}}
        \end{equation*}

        In this case, we have $\beta = 1$ (since the question's statement asks us
        for the F1-measure). As such, we'll have the following:

        \begin{equation*}
          F_{1,a} = \frac{2 \cdot \operatorname{precision}_a \cdot \operatorname{recall}_a}
          {\operatorname{precision}_a + \operatorname{recall}_a}
          = \frac{2 \cdot \frac{1}{3} \cdot \frac{1}{3}}{\frac{1}{3} + \frac{1}{3}}
          = \frac{1}{3}, \quad
          F_{1,b} = \frac{2 \cdot \frac{1}{4} \cdot \frac{1}{3}}{\frac{1}{4} + \frac{1}{3}}
          = \frac{2}{7}, \quad
          F_{1,c} = \frac{2 \cdot \frac{2}{3} \cdot \frac{1}{2}}{\frac{2}{3} + \frac{1}{2}}
          = 0.5714
        \end{equation*}

        Finally, let's talk about the \textbf{random classifier}. A random classifier
        is a classifier that makes random predictions, and is usually used as a baseline
        for comparison with other classifiers. In this case, we have three classes, $A$, $B$ and $C$,
        therefore the random classifier will predict each class with probability $\frac{1}{3}$.
        As such, the accuracy of the random classifier is also $\frac{1}{3}$.

        \textit{The comments below are probably wrong, unsure currently.}

        % TODO: check no HD

        Regarding both sensitivity and precision, we have the following (considering
        $X$ as the class predicted by the random classifier, $\overline{X}$ as all
        the remaining classes, and $p = P(X)$):

        \begin{equation*}
          \operatorname{recall}(X) = \frac{TX}{TX + F\overline{X}} = \frac{p \cdot \#X}{p \cdot \#X + (1 - p) \#X} = p, \quad
          \operatorname{precision}(X) = \frac{TX}{TX + FX} = \frac{p \cdot \#X}{p \cdot \#X + (1 - p) \#\overline{X}}
        \end{equation*}

        Putting it into words, $TX$ is the number of correct $X$ predictions - considering
        a random classifier, and $\#X$ amount of $X$ instances, there'll be $p \cdot \#X$ correct
        $X$ predictions, of course.

        $F\overline{X}$ is the number of incorrect non-$X$ guesses - that is, the
        amount of times we should have predicted $X$ but didn't. Thinking about concrete
        examples, in the random classifier in hands, with $p = \nicefrac{1}{3}$, it's
        intuitive that we'll be able to correctly predict the $X$ event $p \cdot \#X = \nicefrac{1}{3} \cdot X$
        times, but we'll also not be able to predict $X$ $1 - p = \nicefrac{2}{3}$ times:
        for each time we predict $X$ correctly, we'll miss two other times.

        Finally, $FX$ is the number of incorrect $X$ guesses - that is, the amount of
        times we should have predicted $\overline{X}$ but didn't. In an analogous way,
        way to $F\overline{X}$, we'll have $FX = (1 - p) \cdot \#\overline{X}$.

        Note that, considering the formulas above, the recall for all classes is given
        by $p = \nicefrac{1}{3}$, while the precision differs between classes:

        \begin{equation*}
          \operatorname{precision}(A) = \operatorname{precision}(B) = \frac{\nicefrac{1}{3} \cdot 3}{\nicefrac{1}{3} \cdot 3 + \nicefrac{2}{3} \cdot 7} = \frac{3}{17}, \quad
          \operatorname{precision}(C) = \frac{\nicefrac{1}{3} \cdot 4}{\nicefrac{1}{3} \cdot 4 + \nicefrac{2}{3} \cdot 6} = \frac{1}{4}
        \end{equation*}

        \question {
  \item Consider a dataset composed by 374 records, described by 6 variables, classified
        according to the following decision tree:

        \begin{figure}[H]
          \centering
          \includesvg[width=0.5\textwidth]{assets/ex-4-tree.svg}
        \end{figure}

        Each leaf in the tree shows the label, number of classified records with the
        label, and total number of observations in the leaf. The positive class is
        the minority class.

        \begin{enumerate}
          \item Compute the confusion matrix.
          \item Compare the accuracy of the given tree versus a pruned tree, with
                only two nodes. Is there any evidence towards overfitting?
          \item Are decision trees learned from high-dimensional data susceptible
                to underfitting? Why does an ensemble of DTs minimize this problem?
        \end{enumerate}
        }

        By looking at the question statement's figure, we can easily build its respective
        confusion matrix:

        \begin{figure}[H]
          \centering
          \includesvg[width=0.4\textwidth]{assets/ex-4-cm.svg}
        \end{figure}

        By consulting the confusion matrix above, we can see that the accuracy of the
        given tree is given by (note that it's the equivalent of dividing the matrix's
        trace by the sum of all its elements):

        \begin{equation*}
          \operatorname{accuracy} = \frac{TA + TB}{TA + TB + FA + FB} = \frac{114 + 236}{114 + 236 + 6 + 18} = 0.936
        \end{equation*}

        By pruning the tree to only two decision nodes, considering that the second
        decision node already has an $A$-related leaf, every leaf in the now-pruned
        path will belong to a newly created $B$-related leaf. As such, the decision tree
        now looks like this:

        \begin{figure}[H]
          \centering
          \includesvg[width=0.5\textwidth]{assets/ex-4-pruned-tree.svg}
        \end{figure}

        The new confusion matrix and accuracy score are as follows:

        \begin{figure}[H]
          \centering
          \includesvg[width=0.4\textwidth]{assets/ex-4-cm-2.svg}
        \end{figure}

        \begin{equation*}
          \operatorname{accuracy} = \frac{TA + TB}{TA + TB + FA + FB} = \frac{85 + 242}{85 + 242 + 0 + 47} = 0.874
        \end{equation*}

        The accuracy score of the pruned tree is lower than the one of the original
        tree - here, less complexity in the DT ends up being disadvantageous for the
        model. We can't exactly say that there's evidence towards overfitting, though,
        since we have no validation/testing sets to compare the two models' performance
        on unseen data.

        \href{https://towardsdatascience.com/basic-ensemble-learning-random-forest-adaboost-gradient-boosting-step-by-step-explained-95d49d1e2725}{This}
        article gives a good explanation on why tree ensembles (and random forests
        specifically) are less susceptible to overfitting than single decision trees.

\end{enumerate}

\end{document}