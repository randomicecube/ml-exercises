\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=1.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage{svg}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage{nicefrac}
\usepackage{multirow}
\usepackage[most]{tcolorbox}
\usepackage[colorlinks=true]{hyperref}

\setlength{\droptitle}{-6em}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{bg}{rgb}{1,0.96,0.9}

\lstdefinestyle{mystyle}{
  commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2
}

% \tcbset{enlarge left by=-0.8cm,left=1.2cm,enlarge right by=-2cm,right=0.8cm}

\lstset{
  style=mystyle,
  inputencoding=utf8,
  extendedchars=true,
}

\newcommand{\question}[1]{\begin{tcolorbox}[enhanced jigsaw,colback=bg,boxrule=0pt,arc=1pt,halign=center] #1 \end{tcolorbox}}

\begin{document}

The $k$NN, or $k$-nearest neighbors algorithm, is a simple supervised learning algorithm,
which works under the assumption that samples with similar features are more likely to
share the same label. The algorithm is very straight-forward, although differing
between trying to predict categoric and numeric labels: using a given distance
metric (such as Euclidean, Manhattan, etc.), the algorithm finds the $k$ closest samples to
the sample we want to classify, and assigns the label that:

\begin{enumerate}
  \item is most common (i.e. the mode) among the $k$ neighbors, in the case of categoric labels;
  \item is the average of the labels of the $k$ neighbors, in the case of numeric labels.
\end{enumerate}

\begin{enumerate}[leftmargin=\labelsep]
  \question {
  \item Considering the following data set:

        \begin{table}[H]
          \centering
          \begin{tabular}{c|cc|cc}
                  & $y_1$ & $y_2$ & $z_1$ & $z_2$ \\ \hline
            $x_1$ & 1     & 1     & A     & 1.4   \\
            $x_2$ & 2     & 1     & B     & 0.5   \\
            $x_3$ & 2     & 3     & B     & 2     \\
            $x_4$ & 3     & 3     & B     & 2.2   \\
            $x_5$ & 2     & 2     & A     & 0.7   \\
            $x_6$ & 1     & 2     & A     & 1.2
          \end{tabular}
        \end{table}

        Assuming $k$NN, with $k=3$ applied within a leave-one-out schema:

        \begin{enumerate}
          \item Considering an $z_1$ categoric output variable and the Euclidean distance,
                provide the prediction for $x_1$.
          \item Considering an $z_2$ numeric output variable and the cosine similarities,
                provide the mean regression estimate for $x_1$.
          \item Considering a weighted-distance $k$NN, with Manhattan distance, identify both the
                \textbf{weighted-mode} estimate of $x_1$ for a $z_1$ outcome and the
                \textbf{weighted-mean} estimate of $x_1$ for a $z_2$ outcome.
        \end{enumerate}
        }

        Here, working with a leave-one-out schema means that, for each sample, we
        can pick its $k$ neighbors from the pool of all the other samples, excluding
        itself (which, in theory, is always its closest neighbor, with null distance).

        \begin{enumerate}
          \item {
                The Euclidean distance is defined as the square root of the sum of the squared
                differences between the features of the two samples:

                \begin{equation*}
                  d(x_i, x_j) = \sqrt{\sum_{l=1}^p (x_{i}^{(l)} - x_{j}^{(l)})^2}
                \end{equation*}

                Since we're working with a leave-one-out schema, and trying to estimate
                the $z_1$ label for $x_1$, we can pick the $k$ neighbors from all
                samples except $x_1$. Below are illustrated the Euclidean distances
                between those samples and $x_1$, with the $k$ closest neighbors highlighted
                in teal:

                \begin{align*}
                  d(x_1, x_2) = \sqrt{(1 - 2)^2 + (1 - 1)^2} = \textcolor{teal}{1}
                \end{align*}
                \begin{align*}
                  d(x_1, x_3) = \sqrt{5}  , \quad
                  d(x_1, x_4) = 2 \sqrt{2}, \quad
                  d(x_1, x_5) = \textcolor{teal}{\sqrt{2}}  , \quad
                  d(x_1, x_6) = \textcolor{teal}{1}
                \end{align*}

                Knowing the $k$ closest neighbors, we can now estimate the $z_1$ label
                for $x_1$, by picking the most common label among them. In this case,
                the estimated label will be $\mathbf{mode} (B, A, A) = A$.
                }
          \item {
                Here, instead of the usual Euclidean/Manhattan distance, we're using the
                cosine similarities, which are defined as the cosine of the angle between
                the two vectors of features. As we know since high school, the cosine of
                the angle between two vectors is equal to the dot product of the two
                vectors divided by the product of their norms:

                \begin{equation*}
                  \cos(\vec{a}, \vec{b}) = \frac{\vec{a} \cdot \vec{b}}{\|\vec{a}\| \|\vec{b}\|}
                \end{equation*}

                Here, each sample is essentially a vector of features, hence the usage
                of this distance metric. The higher the cosine similarity, the closer
                the two vectors are, and vice versa: the cosine is equal to 1 when the
                two vectors are identical, and 0 when they are orthogonal.

                Let's now compute the cosine similarities between $x_1$ and the other
                samples, picking the $k$ closest neighbors (once again, in teal):

                \begin{align*}
                  \cos(x_1, x_2) = \frac{1 \cdot 2 + 1 \cdot 1}{\sqrt{2} \sqrt{5}} = \frac{3}{\sqrt{10}} = 0.94868, \quad
                  \cos(x_1, x_3) = \frac{1 \cdot 2 + 1 \cdot 3}{\sqrt{2} \sqrt{13}} = \frac{5}{\sqrt{26}} = \textcolor{teal}{0.98058}
                \end{align*}

                \begin{align*}
                  \cos(x_1, x_4) = \frac{1 \cdot 3 + 1 \cdot 3}{\sqrt{2} \sqrt{18}} = \frac{6}{\sqrt{36}} = \textcolor{teal}{1}, \quad
                  \cos(x_1, x_5) = \frac{1 \cdot 2 + 1 \cdot 2}{\sqrt{2} \sqrt{8}} = \frac{4}{\sqrt{16}} = \textcolor{teal}{1}
                \end{align*}

                \begin{align*}
                  \cos(x_1, x_6) = \frac{1 \cdot 1 + 1 \cdot 2}{\sqrt{2} \sqrt{5}} = \frac{3}{\sqrt{10}} = 0.94868
                \end{align*}

                Note, as mentioned above, that the closest neighbors here are the ones
                with a higher cosine similarity: $x_3$, $x_4$, and $x_5$. Since we're
                working with numeric labels for $z_2$, we can now estimate the mean
                regression estimate for $x_1$ by averaging the values of $z_2$ for those
                samples. In this case, the estimated value will be $\mathbf{mean} (2, 2.2, 0.7) = 1.6(3)$.
                }
          \item {
                Note that, here, we're working with a weighted-distance $k$NN, which means
                that we're assigning different weights to each sample based on its distance
                from the sample we're trying to estimate - a closer neighbor will have a
                bigger impact on the final estimate than a further neighbor. Note, also,
                that we're now working with the Manhattan distance, which is defined as
                the sum of the absolute differences between the features of the two samples:

                \begin{equation*}
                  d(x_i, x_j) = \sum_{l=1}^p |x_{i}^{(l)} - x_{j}^{(l)}|
                \end{equation*}

                Let's now compute the Manhattan distances between $x_1$ and the other
                samples, picking the $k$ closest neighbors (once again, in teal):

                \begin{align*}
                  d(x_1, x_2) = |1 - 2| + |1 - 1| = \textcolor{teal}{1}
                \end{align*}

                \begin{align*}
                  d(x_1, x_3) = 3, \quad
                  d(x_1, x_4) = 4, \quad
                  d(x_1, x_5) = \textcolor{teal}{2}, \quad
                  d(x_1, x_6) = \textcolor{teal}{1}
                \end{align*}

                Now, the fun part begins: we have to correctly weigh each neighbor's label
                to estimate the label for $x_1$. We can do this by assigning a weight to
                each neighbor, based on its distance from $x_1$. The most common way to
                do this is by using the inverse of the distance. Regarding the weighted
                mode estimate of $x_1$ for $z_1$:

                \begin{align*}
                  \hat{z}_1 = \mathbf{weighted\_mode} (\nicefrac{1}{1} B, (\nicefrac{1}{1} + \nicefrac{1}{2}) A) = A
                \end{align*}

                The mean, of course, will take into account for the denominator the
                weights of each neighbor, instead of the amount of neighbors:

                \begin{align*}
                  \hat{z}_2 = \mathbf{weighted\_mean} = \frac{\nicefrac{1}{1} \cdot 0.5 + \nicefrac{1}{2} \cdot 0.7 + \nicefrac{1}{1} \cdot 1.2}{\nicefrac{1}{1} + \nicefrac{1}{2} + \nicefrac{1}{1}} = 0.82
                \end{align*}
                }
        \end{enumerate}

        \question {
  \item Consider the following training data set:

        \begin{table}[H]
          \centering
          \begin{tabular}{c|c|c|c}
                  & $y_1$ & $y_2$ & $z$ \\ \hline
            $x_1$ & 1     & 1     & 1.4 \\
            $x_2$ & 2     & 1     & 0.5 \\
            $x_3$ & 1     & 3     & 2   \\
            $x_4$ & 3     & 3     & 2.5
          \end{tabular}
        \end{table}

        \begin{enumerate}
          \item Find the closed form solution for a linear regression, minimizing the
                sum of squared errors.
          \item Predict the target value for the query vector $x_{new} = \begin{bmatrix} 2 & 3 \end{bmatrix}^T$.
          \item Sketch the predicted three-dimensional hyperplane.
          \item Compute both the MSE and MAE produced by the linear regression.
          \item Are there biases on the residuals against any of the input variables?
          \item Compute the closed form solution, considering Ridge regularization term with $\lambda = 0.2$.
          \item Compare the hyperplanes obtained utilizing ordinary least squares and ridge regression.
          \item Why is the Lasso regression usually preferred over Ridge regression for data spaces with a larger number of features?
        \end{enumerate}
        }

        In linear regression, we're trying to learn a linear function that maps the
        input features to the target variable. In other words, we're trying to find
        a function $f$ such that:

        \begin{equation*}
          f(x) = \sum_{i=1}^p w_i x_i + b
        \end{equation*}

        Here, the \textbf{bias}, $b$, is commonly written as $w_0$, and the
        \textbf{weights}, $w_i$, are the coefficients of the linear function - the bias
        is the intercept of the function, allowing us to effectively shift the
        function up or down.

        Note that we always want to make the best predictions possible, of course:
        for that purpose, we'll want our function $f$ to be as close as possible to
        the actual target values. In other words, we want to minimize the error
        between the predicted values and the actual values.
        The most common way to do this is by minimizing the sum of squared errors
        (SSE), which is defined as:

        \begin{equation*}
          \text{SSE} = (XW - Z)^T (XW - Z) = (\hat{Z} - Z)^T (\hat{Z} - Z)
        \end{equation*}

        Considering that we're trying to minimize the error, we can take the derivative
        of the SSE with respect to the weights and set it to zero:

        \begin{equation*}
          \frac{\partial \text{SSE}}{\partial w_i} = 0 \leftrightarrow
          \frac{\partial}{\partial w_i} (\hat{Z} - Z)^T (\hat{Z} - Z) = 0
        \end{equation*}

        After performing some maniacal algebra, we can find the closed form solution
        for the weights:

        \begin{equation*}
          W = (X^T X)^{-1} X^T Z
        \end{equation*}

        \begin{enumerate}
          \item {
                We've derived the closed form solution for the weights, above, so most
                of our work is done here. Considering the training data set, we can
                say that $X$ and $Z$ are the following matrices (note that $X$'s first
                column is all ones, letting the neutral element for the bias, $w_0$,
                shift the function up or down):

                \begin{equation*}
                  X = \input{aux-matrices/ex-2/X}, \quad
                  Z = \input{aux-matrices/ex-2/Z}
                \end{equation*}
                }

                After plugging these matrices into the closed form solution, we gather
                the following weight vector (\texttt{numpy} calculations in the notebook):

                \begin{equation*}
                  W = \input{aux-matrices/ex-2/W}
                \end{equation*}
          \item {
                As we have mentioned in the motivation for this exercise, the predictions,
                $f(x) = \hat{z}$, are given by the following equation:

                \begin{equation*}
                  \hat{z} = \sum_{i=1}^p w_i x_i + w_0
                \end{equation*}

                Considering an initial bias of zero (since no bias is given in the
                question's statement), we can say that:

                \begin{equation*}
                  \hat{z} = W^T \cdot x_{new}
                  = \input{aux-matrices/ex-2/W.tex}^T \cdot \input{aux-matrices/ex-2/x_new.tex}
                  = 2.25
                \end{equation*}
                }
          \item {
                The three-dimensional hyperplane for the linear regression in question
                is the one defined by the following equation:

                \begin{equation*}
                  \hat{z} = \input{aux-matrices/ex-2/W.tex}^T \cdot x
                  = 0.275 + 0.02x_1 + 0.645x_2
                \end{equation*}
                }

                With the aid of Python's \texttt{matplotlib} library, we can plot the
                hyperplane in three dimensions, as shown in the figure below:

                \begin{figure}[H]
                  \centering
                  \includesvg[width=0.75\textwidth]{assets/ex-2/3d-hyperplane.svg}
                \end{figure}
          \item {
                We're asked to compute both the MSE (Mean Squared Error) and the MAE
                (Mean Absolute Error) produced by the linear regression in hands.
                They're defined as follows:

                \begin{align*}
                  \text{MSE} & = \frac{1}{n} \sum_{i=1}^n (\hat{z}_i - z_i)^2 \\
                  \text{MAE} & = \frac{1}{n} \sum_{i=1}^n |\hat{z}_i - z_i|
                \end{align*}

                We'll need, of course, to compute the estimates, $\hat{z}$, for each
                of the training set's original samples. We can do this by plugging the
                training set's input vectors into the linear regression's equation:

                \begin{equation*}
                  \hat{z} = \input{aux-matrices/ex-2/W.tex}^T \cdot x
                \end{equation*}

                The obtained estimates are the following:

                \begin{equation*}
                  \hat{z} = \input{aux-matrices/ex-2/estimates}
                \end{equation*}

                With these estimates, we can compute the MSE and MAE:

                $$
                  \mathbf{MSE} = \frac{1}{4} \sum_{i=1}^4 (\hat{z}_i - z_i)^2
                  = \frac{1}{4} \left( (0.94 - 1.4)^2 + \cdots + (2.27 - 2.5)^2 \right)
                  = 0.0.13225
                $$

                $$
                  \mathbf{MAE} = \frac{1}{4} \sum_{i=1}^4 |\hat{z}_i - z_i|
                  = \frac{1}{4} \left( |0.94 - 1.4| + \cdots + |2.27 - 2.5| \right)
                  = 0.345
                $$
                }
          \item {
                TODO
                }
          \item {
                The Ridge regularization technique aims to \textbf{tune} a linear regression model,
                by adding a penalty term to the SSE. This penalty, usually denoted by $\lambda$,
                changes the closed form solution for the weights, $W$, to the following:

                \begin{equation*}
                  W = (X^T X + \lambda I)^{-1} X^T Z
                \end{equation*}

                where $I$ is the identity matrix. The regularization term, $\lambda$,
                is a hyperparameter that controls the amount of regularization applied
                to the model, in order to avoid overfitting. The larger the value of
                $\lambda$, the more the regression will shift from its original solution.

                With $\lambda=0.2$, as given by the question's statement, we can compute
                the new weights, $W$, as follows:

                \begin{equation*}
                  W = (X^T X + 0.2 I)^{-1} X^T Z = \input{aux-matrices/ex-2/W_ridge}
                \end{equation*}
                }
          \item {
                The Ridge regression's hyperplane is the one shown in the figure below:

                \begin{figure}[H]
                  \centering
                  \includesvg[width=0.5\textwidth]{assets/ex-2/3d-hyperplane-ridge.svg}
                \end{figure}

                The norm of the vector describing the model's hyperplane is, as expected,
                smaller than the one obtained in the previous question. This is due to
                the regularization term, which aims precisely to reduce the model's
                weights' magnitude.
                }
          \item {
                LASSO, or Least Absolute Shrinkage and Selection Operator, is another
                regularization technique that aims to tune a linear regression model.
                It differs from Ridge regression in that it adds a penalty term to the
                SSE, but this term is the sum of the absolute values of the weights,
                instead of the sum of their squares.
                With LASSO, the coefficients are shrunk towards zero, which means that
                some of them will be reduced to zero, effectively removing them from the
                model - this is a so-called \textbf{feature selection}, since it'll
                happen to the least relevant features of the model. Ridge regression,
                on the other hand, will only reduce the weights' magnitude, but will
                never set them to zero. This way, LASSO is more likely to produce a
                sparse (and better) model, with fewer features, than Ridge regression,
                regarding data sets with a large number of features.
                }
        \end{enumerate}

        \question {
  \item Considering the following training data, with $z$ as an ordinal variable:

        \begin{table}[H]
          \centering
          \begin{tabular}{c|c|c|c}
                  & $y_1$ & $y_2$ & z \\ \hline
            $x_1$ & 1     & 1     & 1 \\
            $x_2$ & 2     & 1     & 1 \\
            $x_3$ & 1     & 3     & 0 \\
            $x_4$ & 3     & 3     & 0
          \end{tabular}
        \end{table}

        \begin{enumerate}
          \item Find a linear regression using the closed form solution.
          \item Assuming an output threshold $\theta = 0.5$, provide the predicted class for $x_{new} = \begin{bmatrix} 2 & 2.5 \end{bmatrix}^T$.
        \end{enumerate}
        }

        \begin{enumerate}
          \item {}
          \item {}
        \end{enumerate}

        \question {
  \item Considering the data below to learn the following model, compare:

        $$
          z = w_1 y_1 + w_2 y_2 + \epsilon, \epsilon \sim \mathcal{N}(0, 0.1)
        $$

        \begin{table}[H]
          \centering
          \begin{tabular}{c|c|c|c}
                  & $y_1$ & $y_2$ & z \\ \hline
            $x_1$ & 3     & -1    & 2 \\
            $x_2$ & 4     & 2     & 1 \\
            $x_3$ & 2     & 2     & 1
          \end{tabular}
        \end{table}

        \begin{enumerate}
          \item $w = \begin{bmatrix} w_1 & w_2 \end{bmatrix}^T$, using an MLE approach.
          \item $w$ using the Bayesian approach, assuming $p(w) = N(w \mid \mu = [0, 0], \Sigma = \begin{bmatrix} 0.2 & 0 \\ 0 & 0.2 \end{bmatrix})$.
        \end{enumerate}
        }

        \begin{enumerate}
          \item {}
          \item {}
        \end{enumerate}

        \question {
  \item Identify a transformation to aid linearly modelling the data set above.
        Sketch the predicted surface.
        }

        \question {
  \item Consider both logarithmic and quadratic transformations:

        $$
          \phi_1(x_1) = \log(x_1), \quad \phi_2(x_2) = x_2^2
        $$

        \begin{enumerate}
          \item Plot both of the closed form regressions.
          \item Which transformation minimizes the sum of squared errors on the original data?
        \end{enumerate}
        }

        \begin{enumerate}
          \item {}
          \item {}
        \end{enumerate}

        \question {
  \item Select the criteria promoting a smoother regression model:

        \begin{enumerate}
          \item Applying Ridge and Lasso regularizations to linear regression models.
          \item Increasing the depth of a decision tree regressor.
          \item Increasing the $k$ parameter of a $k$NN regressor.
          \item Parametrizing a $k$NN regressor with uniform weights, instead of the default distance-based weights.
        \end{enumerate}
        }

\end{enumerate}

\end{document}