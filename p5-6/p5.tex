\documentclass[12pt]{article}
\usepackage[paper=letterpaper,margin=1.5cm]{geometry}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amsfonts}
\usepackage{newtxtext, newtxmath}
\usepackage{enumitem}
\usepackage{titling}
\usepackage{svg}
\usepackage{xcolor}
\usepackage{listings}
\usepackage{float}
\usepackage{nicefrac}
\usepackage[most]{tcolorbox}
\usepackage[colorlinks=true]{hyperref}

\setlength{\droptitle}{-6em}

\definecolor{codegreen}{rgb}{0,0.6,0}
\definecolor{codegray}{rgb}{0.5,0.5,0.5}
\definecolor{codepurple}{rgb}{0.58,0,0.82}
\definecolor{backcolour}{rgb}{0.95,0.95,0.92}
\definecolor{bg}{rgb}{1,0.96,0.9}

\lstdefinestyle{mystyle}{
  commentstyle=\color{codegreen},
  keywordstyle=\color{magenta},
  numberstyle=\tiny\color{codegray},
  stringstyle=\color{codepurple},
  basicstyle=\ttfamily\footnotesize,
  breakatwhitespace=false,
  breaklines=true,
  captionpos=b,
  keepspaces=true,
  numbers=left,
  numbersep=5pt,
  showspaces=false,
  showstringspaces=false,
  showtabs=false,
  tabsize=2
}

% \tcbset{enlarge left by=-0.8cm,left=1.2cm,enlarge right by=-2cm,right=0.8cm}

\lstset{
  style=mystyle,
  inputencoding=utf8,
  extendedchars=true,
}

\begin{document}

\textit{Note: intermediate steps were calculated using \texttt{numpy} and, in general, will not be shown.}

\begin{enumerate}[leftmargin=\labelsep]
  \begin{tcolorbox}[enhanced jigsaw,colback=bg,boxrule=0pt,arc=1pt,halign=center]
    \item Considering the following linearly separable training data:

    \begin{table}[H]
      \centering
      \begin{tabular}{c|c|c|c|c}
              & $y_1$ & $y_2$ & $y_3$ & $z$ \\ \hline
        $x_1$ & 0     & 0     & 0     & -1  \\
        $x_2$ & 0     & 2     & 1     & 1   \\
        $x_3$ & 1     & 1     & 1     & 1   \\
        $x_4$ & 1     & -1    & 0     & -1
      \end{tabular}
    \end{table}

    Given the perceptron learning algorithm with a learning rate $\eta = 1$,
    sign activation and all weights initialized to one (including the bias):

    \begin{enumerate}
      \item Considering $y_1$ and $y_2$, apply the algorithm until convergence.
            Draw the separation hyperplane.
      \item Considering all input variables, apply one epoch of the algorithm.
            Do weights change for an additional epoch?
      \item Identify the perceptron output for $x_{new}$ = $\begin{bmatrix} 0 & 0 & 1 \end{bmatrix}^T$.
      \item What happens if we replace the sign function with the step function? Specifically,
            how would you change $\eta$ to ensure the same results?
    \end{enumerate}
  \end{tcolorbox}

  \begin{enumerate}
    \item {}
    \item {}
    \item {}
    \item {}
  \end{enumerate}

  \begin{tcolorbox}[enhanced jigsaw,colback=bg,boxrule=0pt,arc=1pt,halign=center]
    \item Show graphically, instantiating the parameters, that a perceptron:

    \begin{enumerate}
      \item Can learn the NOT, AND and OR logical functions.
      \item Can't learn the XOR logical function (for two inputs).
    \end{enumerate}
  \end{tcolorbox}

  \begin{enumerate}
    \item {}
    \item {}
  \end{enumerate}

  \begin{tcolorbox}[enhanced jigsaw,colback=bg,boxrule=0pt,arc=1pt,halign=center]
    \item Let us consider the following activation function:

    \begin{equation*}
      \hat{z}(x, w) = \frac{1}{1 + e^{-2 w x}}
    \end{equation*}

    Consider also the half sum of squared errors as the loss function:

    \begin{equation*}
      E(w) = \nicefrac{1}{2} \sum_{i=1}^N (z_i - \hat{z}(x_i, w))^2
    \end{equation*}

    \begin{enumerate}
      \item Determine the gradient descent learning rule for this unit.
      \item Compute the first gradient descent update, assuming an initialization of all ones.
      \item Compute the first stochastic gradient descent update assuming an initialization of all ones.
    \end{enumerate}
  \end{tcolorbox}

  \begin{enumerate}
    \item {}
    \item {}
    \item {}
  \end{enumerate}

  \begin{tcolorbox}[enhanced jigsaw,colback=bg,boxrule=0pt,arc=1pt,halign=center]
    \item Let us consider the following activation function:

    \begin{equation*}
      \hat{z}(x, w) = \frac{1}{1 + e^{-w x}}
    \end{equation*}

    Here, we'll be using the cross-entropy loss function:

    \begin{equation*}
      E(w) = - \sum_{i=1}^N z_i \log \hat{z}(x_i, w) + (1 - z_i) \log (1 - \hat{z}(x_i, w))
    \end{equation*}

    \begin{enumerate}
      \item Determine the gradient descent learning rule for this unit.
      \item Compute the first gradient descent update, assuming an initialization of all ones.
      \item Compute the first stochastic gradient descent update assuming an initialization of all ones.
    \end{enumerate}
  \end{tcolorbox}

  \begin{enumerate}
    \item {}
    \item {}
    \item {}
  \end{enumerate}

  \begin{tcolorbox}[enhanced jigsaw,colback=bg,boxrule=0pt,arc=1pt,halign=center]
    \item Consider now the activation function described in the previous exercise,
    paired with the half sum of squared errors loss function.

    \begin{enumerate}
      \item Determine the gradient descent learning rule for this unit.
      \item Compute the stochastic gradient descent update for input $x_{new}$ = $\begin{bmatrix} 1 & 1 \end{bmatrix}^T$,
            $z_{new} = 0$, with initial weights $w = \begin{bmatrix} 0 & 1 & 0 \end{bmatrix}^T$
            and learning rate $\eta = 2$.
    \end{enumerate}
  \end{tcolorbox}

  \begin{enumerate}
    \item {}
    \item {}
  \end{enumerate}

  \begin{tcolorbox}[enhanced jigsaw,colback=bg,boxrule=0pt,arc=1pt,halign=center]
    \item Consider the sum squared and cross-entropy loss functions. Any stands out?
    What changes when one changes the loss function?
  \end{tcolorbox}

\end{enumerate}

\end{document}